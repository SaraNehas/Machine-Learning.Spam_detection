{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################---LIBRARIES---###################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################---FUNCTIONS---###################\n",
    "\n",
    "def add_feature(X, feature_to_add):\n",
    "    \"\"\"\n",
    "    Returns sparse feature matrix with added feature.\n",
    "    feature_to_add can also be a list of features.\n",
    "    \"\"\"\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################---IMPORTING AND SPLITTING DATA---###################\n",
    "\n",
    "spam_data = pd.read_csv('spam.csv')\n",
    "\n",
    "spam_data['target'] = np.where(spam_data['target']=='spam',1,0)\n",
    "spam_data.head(10)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(spam_data['text'],spam_data['target'],random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mSOME INFORMATION ABOUT DATA:\u001b[0m\n",
      "\n",
      "13.41% of the texts are SPAM texts.\n",
      "\n",
      "The longest word in the whole data is com1win150ppmx3age16subscription \n",
      "\n",
      "The 20 words with SMALLEST tfidf Coefs:\n",
      "sympathetic     0.074475\n",
      "healer          0.074475\n",
      "aaniye          0.074475\n",
      "dependable      0.074475\n",
      "companion       0.074475\n",
      "listener        0.074475\n",
      "athletic        0.074475\n",
      "exterminator    0.074475\n",
      "psychiatrist    0.074475\n",
      "pest            0.074475\n",
      "determined      0.074475\n",
      "chef            0.074475\n",
      "courageous      0.074475\n",
      "stylist         0.074475\n",
      "psychologist    0.074475\n",
      "organizer       0.074475\n",
      "pudunga         0.074475\n",
      "venaam          0.074475\n",
      "diwali          0.091250\n",
      "mornings        0.091250\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "The 20 words with LARGEST tfidf Coefs: \n",
      "146tf150p    1.000000\n",
      "havent       1.000000\n",
      "home         1.000000\n",
      "okie         1.000000\n",
      "thanx        1.000000\n",
      "er           1.000000\n",
      "anything     1.000000\n",
      "lei          1.000000\n",
      "nite         1.000000\n",
      "yup          1.000000\n",
      "thank        1.000000\n",
      "ok           1.000000\n",
      "where        1.000000\n",
      "beerage      1.000000\n",
      "anytime      1.000000\n",
      "too          1.000000\n",
      "done         1.000000\n",
      "645          1.000000\n",
      "tick         0.980166\n",
      "blank        0.932702\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "\n",
      "The mean nbr of words in NON-spam texts is 71.02.\n",
      "The mean nbr of words in spam texts is 138.87.\n",
      "\n",
      "The mean nbr of digits in NON-spam texts is 0.30.\n",
      "The mean nbr of digits in spam texts is 15.76.\n",
      "\n",
      "The mean nbr of non-word characters in NON-spam texts is 17.29.\n",
      "The mean nbr of non-word characters in spam texts is 29.04.\n"
     ]
    }
   ],
   "source": [
    "###################---GET TO KNOW DATA---###################\n",
    "\n",
    "print('\\033[1mSOME INFORMATION ABOUT DATA:\\033[0m\\n')\n",
    "\n",
    "###---Percentage of spam texts---### \n",
    "num_spam=spam_data['target'].value_counts()[1]\n",
    "num_items=len(spam_data['target'])\n",
    "print('{:3.2f}% of the texts are SPAM texts.\\n'.format(num_spam/num_items*100))\n",
    "\n",
    "\n",
    "###---The longest word---### \n",
    "vect = CountVectorizer().fit(X_train)\n",
    "vect_feat=vect.get_feature_names()\n",
    "len_tok = [(w,len(w)) for w in vect_feat ]\n",
    "sort_tok=sorted(len_tok,reverse=True, key=lambda x: x[1])\n",
    "print('The longest word in the whole data is',sort_tok[0][0],'\\n')\n",
    "\n",
    "\n",
    "###---20 largest and 20 smallest tfidfs---###  \n",
    "vect = TfidfVectorizer().fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "    \n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "sorted_tfidf = X_train_vectorized.max(0).toarray()[0]\n",
    "    \n",
    "sorted_tfidf_index = sorted_tfidf.argsort()\n",
    "    \n",
    "smallest_df=pd.DataFrame(sorted_tfidf[sorted_tfidf_index[:20]])\n",
    "smallest_df.set_index(feature_names[sorted_tfidf_index[:20]],inplace=True)\n",
    "    \n",
    "largest_df=pd.DataFrame(sorted_tfidf[sorted_tfidf_index[:-21:-1]])\n",
    "largest_df.set_index(feature_names[sorted_tfidf_index[:-21:-1]],inplace=True)\n",
    "\n",
    "print('The 20 words with SMALLEST tfidf Coefs:\\n{}\\n'.format(smallest_df.iloc[:,0]))\n",
    "print('The 20 words with LARGEST tfidf Coefs: \\n{}\\n\\n'.format(largest_df.iloc[:,0]))\n",
    "\n",
    "\n",
    "###---Average number of words and digits in spam and non-spam texts---###\n",
    "spam_data['char. count']= [len(sent) for sent in spam_data['text']]\n",
    "spam_data['digit count']= [sum(c.isdigit() for c in s) for s in spam_data['text']]\n",
    "\n",
    "spam=spam_data[spam_data['target']==1]\n",
    "non_spam=spam_data[spam_data['target']==0]\n",
    "# ------\n",
    "mean_spam=np.mean(spam['char. count'])\n",
    "mean_non_spam=np.mean(non_spam['char. count'])\n",
    "\n",
    "print('''The mean nbr of words in NON-spam texts is {0:3.2f}.\n",
    "The mean nbr of words in spam texts is {1:3.2f}.\\n'''.format(mean_non_spam,mean_spam))\n",
    "# ------\n",
    "mean_dspam=np.mean(spam['digit count'])\n",
    "mean_dnon_spam=np.mean(non_spam['digit count'])\n",
    "    \n",
    "print('''The mean nbr of digits in NON-spam texts is {0:3.2f}.\n",
    "The mean nbr of digits in spam texts is {1:3.2f}.\\n'''.format(mean_dnon_spam,mean_dspam))\n",
    "\n",
    "\n",
    "###---Average number of non-word characters in spam and non-spam texts---### \n",
    "mean_Nnon_spam = non_spam.apply(lambda x:len(re.findall(r'\\W',x[0])), axis=1).mean()\n",
    "mean_Nspam = spam.apply(lambda x:len(re.findall(r'\\W',x[0])), axis=1).mean()\n",
    "\n",
    "print('''The mean nbr of non-word characters in NON-spam texts is {0:3.2f}.\n",
    "The mean nbr of non-word characters in spam texts is {1:3.2f}.'''.format(mean_Nnon_spam,mean_Nspam))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTESTING 4 COMBINATIONS OF VECTORIZERS AND ALGORITHMS:\u001b[0m\n",
      "\n",
      "The AUC score using CountVectorizer and Multinomial Naive Bayes is 0.97.\n",
      "\n",
      "The AUC score using TfidfVectorizer and Multinomial Naive Bayes is 0.94.\n",
      "\n",
      "The AUC score using TfidfVectorizer and SVM and adding ONE feature is 0.96.\n",
      "\n",
      "The AUC score using TfidfVectorizer and Logistic Regression and adding TWO feature is 0.97.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###################---MODEL TESTING---###################\n",
    "print('\\033[1mTESTING 4 COMBINATIONS OF VECTORIZERS AND ALGORITHMS:\\033[0m\\n')\n",
    "\n",
    "###---CountVectorizer and MultinomialNB to predict ytest---### \n",
    "vect = CountVectorizer().fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "NB=MultinomialNB(alpha=0.1).fit(X_train_vectorized, y_train)\n",
    "predictions = NB.predict(vect.transform(X_test))\n",
    "\n",
    "print('The AUC score using CountVectorizer and Multinomial Naive Bayes is {:3.2f}.\\n'\n",
    "      .format(roc_auc_score(y_test, predictions)))\n",
    "\n",
    "\n",
    "###---TfidfVectorizer and MultinomialNB to predict ytest---### \n",
    "vect = TfidfVectorizer(min_df=3).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)  \n",
    "NB=MultinomialNB(alpha=0.1).fit(X_train_vectorized, y_train)\n",
    "predictions = NB.predict(vect.transform(X_test))\n",
    "\n",
    "print('The AUC score using TfidfVectorizer and Multinomial Naive Bayes is {:3.2f}.\\n'\n",
    "      .format(roc_auc_score(y_test, predictions)))\n",
    "\n",
    "\n",
    "###---TfidfVectorizer,adding features and SVM to predict ytest---### \n",
    "vect = TfidfVectorizer(min_df=5).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_test_vectorized=vect.transform(X_test)\n",
    "x_len1 = X_train.apply(len)\n",
    "X_train_aug = add_feature(X_train_vectorized, x_len1)\n",
    "    \n",
    "x_len2 = X_test.apply(len)\n",
    "X_test_aug = add_feature(X_test_vectorized, x_len2)\n",
    "    \n",
    "SVC_model=SVC(C=10000,gamma='auto').fit(X_train_aug,y_train)\n",
    "predictions = SVC_model.predict(X_test_aug)\n",
    "\n",
    "print('The AUC score using TfidfVectorizer and SVM and adding ONE feature is {:3.2f}.\\n'\n",
    "      .format(roc_auc_score(y_test, predictions)))\n",
    "\n",
    "\n",
    "###---TfidfVectorizer,adding features and LogisticRegression to predict ytest---### \n",
    "vect = TfidfVectorizer(min_df=5, ngram_range=(1,3)).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "\n",
    "x_len = X_train.apply(len)\n",
    "X_train_aug = add_feature(X_train_vectorized, x_len)\n",
    "x_digit = X_train.apply(lambda x: len(re.sub('\\D','', x)))\n",
    "X_train_aug2 = add_feature(X_train_aug, x_digit)\n",
    "\n",
    "x_len2 = X_test.apply(len)\n",
    "X_test_aug = add_feature(X_test_vectorized, x_len2)\n",
    "x_digit2 = X_test.apply(lambda x: len(re.sub('\\D','', x)))\n",
    "X_test_aug2 = add_feature(X_test_aug, x_digit2)\n",
    "\n",
    "model = LogisticRegression(C=100,solver='liblinear').fit(X_train_aug2, y_train)\n",
    "predictions = model.predict(X_test_aug2)\n",
    "\n",
    "print('The AUC score using TfidfVectorizer and Logistic Regression and adding TWO feature is {:3.2f}.\\n'\n",
    "      .format(roc_auc_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTESTING THE EFFECT OF VECTORIZING BEFORE AND AFTER SPLITTING DATA:\n",
      "\n",
      "\n",
      "--Vectorizing BEFORE splitting--\u001b[0m\n",
      "The AUC score using CountVectorizer before slitting Data 0.98.\n",
      "\n",
      "The 10 words with Smallest features coefficients are:\n",
      "['. ', '..', '? ', ' i', ' y', ' go', ':)', 'he', ' h', ' m'].\n",
      "\n",
      "The 10 words with Largest features coefficients are:\n",
      "['digit_count', 'xt', 'ww', 'ne', 'mob', 'ia', 'co', 'ar', ' x', ' ch'].\n",
      "\n",
      "\n",
      "\u001b[1m--Vectorizing AFTER splitting--\n",
      "\u001b[0m\n",
      "The AUC score using CountVectorizer before slitting Data 0.98.\n",
      "\n",
      "The 10 words with Smallest features coefficients are:\n",
      "['. ', '..', '? ', ' i', ' y', ' go', ':)', ' h', 'he', 'go'].\n",
      "\n",
      "The 10 words with Largest features coefficients are:\n",
      "['digit_count', 'xt', 'ww', 'ne', 'mob', 'ia', 'co', 'ar', ' x', ' ch'].\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###################---TESTING VECTORIZING BEFORE AND AFTER SPLITTING---###################\n",
    "\n",
    "print('\\033[1mTESTING THE EFFECT OF VECTORIZING BEFORE AND AFTER SPLITTING DATA:\\n\\n')\n",
    "\n",
    "###---CountVectorize betfore splitting data---###  \n",
    "print('--Vectorizing BEFORE splitting--\\033[0m')\n",
    "\n",
    "X=spam_data['text']\n",
    "y=spam_data['target']\n",
    "\n",
    "#-Vectorizing first\n",
    "vect = CountVectorizer(min_df=5,analyzer='char_wb',ngram_range=(2,5)).fit(X)\n",
    "X_vectorized = vect.transform(X)\n",
    "\n",
    "#-Adding features\n",
    "x_len = X.apply(len)\n",
    "X_aug = add_feature(X_vectorized, x_len)\n",
    "x_digit = X.apply(lambda x: len(re.sub('\\D','', x)))\n",
    "X_aug2 = add_feature(X_aug, x_digit)\n",
    "x_nonw = X.apply(lambda x: len(re.findall(r'\\W',x[0])))\n",
    "X_aug3 = add_feature(X_aug2, x_nonw)\n",
    "\n",
    "#-Then splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_aug3,y,random_state=0)\n",
    "\n",
    "#-Training model\n",
    "model = LogisticRegression(C=100,solver='liblinear').fit(X_train, y_train)\n",
    "    \n",
    "features=np.array(vect.get_feature_names())\n",
    "New_features=np.append(features,['length_of_doc', 'digit_count', 'non_word_char_count'])\n",
    "    \n",
    "predictions = model.predict(X_test)\n",
    "AUC_Score=roc_auc_score(y_test, predictions)\n",
    "sorted_coef_index=model.coef_[0].argsort()\n",
    "coeff_min=list(New_features[sorted_coef_index[:10]])\n",
    "coeff_max=list(New_features[np.sort(sorted_coef_index[-10:])[::-1]])\n",
    "\n",
    "print('The AUC score using CountVectorizer before slitting Data {:3.2f}.\\n'.format(AUC_Score))\n",
    "print('The 10 words with Smallest features coefficients are:\\n{}.\\n'.format(coeff_min))\n",
    "print('The 10 words with Largest features coefficients are:\\n{}.\\n\\n'.format(coeff_max))\n",
    "\n",
    "\n",
    "\n",
    "###---CountVectorize After splitting data---###  \n",
    "print('\\033[1m--Vectorizing AFTER splitting--\\n\\033[0m')\n",
    "\n",
    "#-Splitting data first\n",
    "X_train, X_test, y_train, y_test = train_test_split(spam_data['text'],spam_data['target'], \n",
    "                                                    random_state=0)\n",
    "\n",
    "#-Then vectorizing\n",
    "vect = CountVectorizer(min_df=5,analyzer='char_wb',ngram_range=(2,5)).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "\n",
    "#-Adding features   \n",
    "x_len = X_train.apply(len)\n",
    "X_train_aug = add_feature(X_train_vectorized, x_len)\n",
    "x_digit = X_train.apply(lambda x: len(re.sub('\\D','', x)))\n",
    "X_train_aug2 = add_feature(X_train_aug, x_digit)\n",
    "x_nonw = X_train.apply(lambda x: len(re.findall(r'\\W',x[0])))\n",
    "X_train_aug3 = add_feature(X_train_aug2, x_nonw)\n",
    "    \n",
    "x_len2 = X_test.apply(len)\n",
    "X_test_aug = add_feature(X_test_vectorized, x_len2)\n",
    "x_digit2 = X_test.apply(lambda x: len(re.sub('\\D','', x)))\n",
    "X_test_aug2 = add_feature(X_test_aug, x_digit2)\n",
    "x_nonw2 = X_test.apply(lambda x: len(re.findall(r'\\W',x[0])))\n",
    "X_test_aug3 = add_feature(X_test_aug2, x_nonw2)\n",
    "\n",
    "#-Training model\n",
    "model = LogisticRegression(C=100,solver='liblinear').fit(X_train_aug3, y_train)\n",
    "    \n",
    "features=np.array(vect.get_feature_names())\n",
    "New_features=np.append(features,['length_of_doc', 'digit_count', 'non_word_char_count'])\n",
    "    \n",
    "predictions = model.predict(X_test_aug3)\n",
    "    \n",
    "AUC_Score=roc_auc_score(y_test, predictions)\n",
    "sorted_coef_index=model.coef_[0].argsort()\n",
    "coeff_min=list(New_features[sorted_coef_index[:10]])\n",
    "coeff_max=list(New_features[np.sort(sorted_coef_index[-10:])[::-1]])\n",
    "\n",
    "print('The AUC score using CountVectorizer before slitting Data {:3.2f}.\\n'.format(AUC_Score))\n",
    "print('The 10 words with Smallest features coefficients are:\\n{}.\\n'.format(coeff_min))\n",
    "print('The 10 words with Largest features coefficients are:\\n{}.\\n'.format(coeff_max))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
